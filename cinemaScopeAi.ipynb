{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "{'genres': ['Drama', 'Adventure', 'Action'], 'budget': '10-25 million USD'}\n"
     ]
    }
   ],
   "source": [
    "genres = ['Drama', 'Adventure', 'Action', 'Crime', 'Comedy', 'Thriller', 'Animation', 'Mystery', 'Romance', 'Biography']\n",
    "budget_lst = [\n",
    "        \"under 1 million USD\",\n",
    "        \"1-10 million USD\",\n",
    "        \"10-25 million USD\",\n",
    "        \"25-50 million USD\",\n",
    "        \"50 million USD or more\"\n",
    "    ]\n",
    "def split_label(path):\n",
    "    filename = os.path.basename(path)\n",
    "    label_part = filename.strip()[0:16]\n",
    "    label = [int(bit) for bit in label_part if bit.isdigit()]\n",
    "    # one hot\n",
    "\n",
    "    return label\n",
    "\n",
    "def decode_label(label_part):\n",
    "    genre_bits = label_part[:10]  # first 10 bits for genres\n",
    "    budget_bits = label_part[10:15]  # next 5 bits for budgets\n",
    "    genres_label = []\n",
    "    budget_label = \"\"\n",
    "    for i in range(10):\n",
    "        if genre_bits[i] == 1:\n",
    "            genres_label.append(genres[i])\n",
    "    for i in range(5):\n",
    "        if budget_bits[i] == 1:\n",
    "            budget_label = budget_lst[i]\n",
    "\n",
    "    return {\n",
    "        'genres': genres_label,\n",
    "        'budget': budget_label\n",
    "    }\n",
    "example_path = \"./database/1110000000 00100 10152.jpg\"\n",
    "example_label = split_label(example_path)\n",
    "print(example_label)\n",
    "print(decode_label(example_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "tf.Tensor(\n",
      "[[1 0 1 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 1 0 0 0 0 1 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 1 0 1 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 1 0 0 1 0 1 0 0 0 0 0 0 0 1]], shape=(32, 15), dtype=int32)\n",
      "277 92 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 08:16:42.484173: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize to the expected input size of your model\n",
    "    image = image / 255.0  # Normalize the image to [0, 1] range\n",
    "    return image, label\n",
    "\n",
    "def create_dataset_from_directory(directory=\"database\"):\n",
    "    # Collect all paths and labels\n",
    "    paths, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):  # Assuming JPEG images\n",
    "                path = os.path.join(root, file)\n",
    "                paths.append(path)\n",
    "                label = split_label(path)\n",
    "                labels.append(label)\n",
    "    # Convert lists to tensor slices\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    image_label_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "\n",
    "    # Map the load and preprocess function\n",
    "    image_label_ds = image_label_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return image_label_ds\n",
    "\n",
    "dataset = create_dataset_from_directory()\n",
    "dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Example iteration over the dataset\n",
    "for images, labels in dataset.take(1):  # Take 1 batch\n",
    "    print(images.shape)  # Output the shape of images, e.g., (32, 224, 224, 3)\n",
    "    print(labels)        # Output the corresponding labels\n",
    "\n",
    "# Split to train valid test\n",
    "train_size = int(0.6 * len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset = dataset.take(train_size)\n",
    "valid_dataset = dataset.skip(train_size).take(valid_size)\n",
    "test_dataset = dataset.skip(train_size + valid_size).take(test_size)\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 base model, pretrained on ImageNet\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze the convolutional base\n",
    "\n",
    "# Add custom layers on top for your specific task\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(15, activation='sigmoid')(x)  # Adjust the number of outputs and activation function based on your task\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(15, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1803s\u001b[0m 7s/step - accuracy: 0.5634 - loss: 0.5690 - val_accuracy: 0.6916 - val_loss: 0.4236 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1936s\u001b[0m 7s/step - accuracy: 0.6720 - loss: 0.4431 - val_accuracy: 0.6940 - val_loss: 0.4117 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1854s\u001b[0m 7s/step - accuracy: 0.6669 - loss: 0.4232 - val_accuracy: 0.6878 - val_loss: 0.4071 - learning_rate: 0.0010\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "# Callback to save the best model\n",
    "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Callback to reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Callback for early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, verbose=1, mode='max', restore_best_weights=True)\n",
    "\n",
    "# Train the model with added early stopping\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation and Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 5s/step - accuracy: 0.6812 - loss: 0.4325\n",
      "Test accuracy: 0.6800804734230042\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)  # Assuming 'test_dataset' is prepared\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
